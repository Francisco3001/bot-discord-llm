 # ğŸ¤– Bot de Discord + LLM Local con Ollama

Este proyecto implementa un bot de Discord que responde preguntas usando un modelo de lenguaje **Mistral** corriendo **localmente con Ollama**. Todo estÃ¡ dockerizado para facilitar la ejecuciÃ³n y portabilidad.


---

## ğŸš€ Â¿QuÃ© hace?

- Escucha mensajes de Discord que comienzan con `!`
- EnvÃ­a el mensaje (con un contexto predefinido) al modelo `mistral`
- Responde en el canal de Discord con la respuesta generada localmente

---

## ğŸ›  Requisitos

- [Docker](https://www.docker.com/)
- [Ollama](https://ollama.com/) si querÃ©s probarlo fuera de Docker

---

## ğŸ³ CÃ³mo correr el proyecto

### 1. Clonar y entrar en el repositorio

```git clone https://github.com/Francisco3001/bot-discord-llm.git```

```cd bot-discord-llm```

###  2. Construir e iniciar todo
```docker compose up --build```

## âš™ï¸ Variables de entorno

Crear un archivo `.env` en la raÃ­z con tu token de Discord:

```DISCORD_TOKEN=TU_TOKEN```


## ğŸ§  Â¿CÃ³mo funciona el contexto?
El archivo `bot/contexto.txt` se envÃ­a en cada mensaje al modelo, antes de la pregunta del usuario. Esto permite personalizar el comportamiento general (por ejemplo: tono tÃ©cnico, estilo simple, etc.).

## ğŸ“Œ Modelos soportados
PodÃ©s reemplazar mistral por cualquier modelo compatible con Ollama, como:
- llama2
- gemma
- codellama
- mixtral
- tinyllama
Para eso, cambia la configuracione en el Dockerfile y el campo "model" de la peticion http en run.py
